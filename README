<!-- vim: syntax=Markdown -->
# CNeuroMod: [HCP test-retest (hcptrt) raw dataset](https://github.com/courtois-neuromod/hcptrt)

This repository contains the raw data of a [CNeuroMod](https://cneuromod.ca) dataset called `hcptrt`, where Participants performed tasks from the [HCP](https://www.humanconnectome.org/) project multiple times across sessions while functional data being acquired in the MRI scanner.

:::{tip} How to cite

If you use this dataset, please cite:

Rastegarnia, S., St-Laurent, M., DuPre, E., Pinsard, B., & Bellec, P. (2023).
_Brain decoding of the Human Connectome Project tasks in a dense individual fMRI dataset_.
**NeuroImage**, 283, 120395. [doi: 10.1016/j.neuroimage.2023.120395](https://doi.org/10.1016/j.neuroimage.2023.120395)

:::


âœ… = availableâ€ƒâ€ƒâ¬œ = collected but not yet integratedâ€ƒâ€ƒâŒ = not collected

| Field       |  Description |
|-------------|--------------|
| Subjects    |  `sub-01` âœ… Â· `sub-02` âœ… Â· `sub-03` âœ… Â· `sub-04` âœ… Â· `sub-05` âœ… Â· `sub-06` âœ…,|
| Duration  |  10â€“13 sessions, ~8 h fMRI per participant |
| Tasks       |  ğŸ­ 7 HCP task localizers (23 conditions) x 13-18 repetitions per participant |
|             |  ğŸ’¤ resting state x 4-6 repetitions per participant|
| Data        | ğŸ§  Neuroimaging (fMRI) âœ… |
|             | ğŸ“Š Behavior     (Event files) âœ… |
|             | ğŸ“ˆ Physiology   (ECG âŒ, pulse âŒ, respiration âŒ, skin conductance âŒ) |
|             | ğŸ‘ï¸ Eyetracking  (Gaze âŒ, pupillometry  âŒ)  |
| Resources   | ğŸ”£ [Access the data on CONP](https://portal.conp.ca/dataset?id=projects/cneuromod) |
|             | ğŸ”£ [Data versioning on GitHub](https://github.com/courtois-neuromod/hcptrt) |
|             | ğŸ“– [Docs](https://docs.cneuromod.ca/en/latest/DATASETS.html#hcptrt) |
|             | ğŸ“– [Data Paper](https://doi.org/10.1016/j.neuroimage.2023.120395) |

## Contributors âœ¨
[Lune Bellec](http://github.com/lunebellec) ğŸ¨ ğŸ“– Â· [Julie Boyle](https://github.com/julieaboyle1) ğŸ”£ ğŸ“† ğŸ“– ğŸ¤” ğŸ¨ Â· [Arnaud Bore](https://github.com/arnaudbore) ğŸ’» ğŸ”£ Â· [Valentina Borghesani](http://valentina.borghesani.org/) ğŸ‘€ ğŸ’¬ ğŸ§‘â€ğŸ« ğŸ“– ğŸ› ğŸ““ Â· [Amal Boukhdhir](https://github.com/bamal) ğŸ”£ ğŸ‘€ ğŸ“– Â· [Elizabeth DuPre](http://github.com/emdupre) ğŸ‘€ ğŸ““ ğŸ› Â· [Basile Pinsard](https://github.com/bpinsard) ğŸ”£ ğŸ‘€ ğŸš§ ğŸ’» Â· Claude Godbout ğŸ”£ Â· Courtois Foundation ğŸ’°

Legend: ğŸ¨ design Â· ğŸ“– documentation Â· ğŸ”£ data Â· ğŸ‘€ review Â· ğŸš§ maintenance Â· ğŸ’» code Â· ğŸ“† project management Â· ğŸ¤” ideas Â· ğŸ’¬ questions Â· ğŸ§‘â€ğŸ« mentoring Â· ğŸ› bug reports Â· ğŸ““ user testing Â· ğŸ’° funding

## Protocol
The fMRI paradigm was based on the HCP task battery, which targets seven cognitive domains that include 23 different experimental task conditions as well as resting-state, described below (text adapted from the [HCP protocol](http://protocols.humanconnectome.org/HCP/3T/task-fMRI-protocol-details.html)). Before each task, participants were given detailed instructions and examples, as well as a practice run. A session was typically composed either of two repetitions of the HCP localizers, or one resting-state run and one HCP localizer. The e-prime scripts for preparation and presentation of the stimuli can be found in the [HCP database](https://db.humanconnectome.org/app/action/ChooseDownloadResources?project=HCP_Resources&resource=Scripts&filePath=HCP_TFMRI_scripts.zip).

Stimuli and e-prime scripts were provided by the Human Connectome Project, U-Minn Consortium (Principal Investigators: David Van Essen and Kamil Ugurbil; 1U54MH091657) funded by the 16 NIH Institutes and Centers that support the NIH Blueprint for Neuroscience Research, and by the McDonnell Center for Systems Neuroscience at Washington University.

## Note on data organization
Functional runs are named `func_sub-<participant>_ses-<sess>_task-<task>_run-<run>`, where the `<participant>` tag includes `sub-01` to `sub-06`. For each functional run, a companion file `_events.tsv` contains the timing and type of events presented to the subject. Session tags `<sess>` are `001`, `002` etc, and the number and composition of sessions vary from subject to subject. The `<task>` tags are `restingstate`, `gambling`, `motor`, `social`, `wm`, `emotion`, `language` and `relational`, as described below. Tasks that were repeated twice have separate `<run>` tags (`01`, `02`).

:::{important}
The duration of BOLD series are slightly varying across participants and repetitions. If consistent length is required by analysis, series can be trimmed at the end to match duration, task being aligned to the first TR.
:::

## Tasks

### [Gambling](http://www.cognitiveatlas.org/task/id/trm_550b5c1a7f4db/)
`gambling` duration: approximately 3 minutes. Participants were asked to guess whether a hidden number (represented by a â€œ?â€ during 1500ms) was above or below 5 [(Delgado et al. 2000)](https://doi.org/10.1016/j.neuroimage.2004.10.002). They indicated their choice using a button press, and were then shown the actual number. If they guessed correctly they were told they won money (+$1.00, `win` trial), if they guessed incorrectly they were told they lost money (-$0.50, `loss` trial), and if the number was exactly 5 they were told that they neither won or lost money ($0, `neutral` trial). Note that no money was actually given to the participants and, as such, this task may not be an accurate reproduction of the HCP protocol. The conditions were presented in blocks of 8 trials of type `reward` (6 `win` trials pseudo randomly interleaved with either 1 `neutral` and 1 `loss` trial, 2 `neutral` trials, or 2 `loss` trials) or of type `punishment` (6 `loss` trials pseudo-randomly interleaved with either 1 `neutral` and 1 `win` trial, 2 `neutral` trials, or 2 `win` trials). There were four blocks per run (2 `reward` and 2 `punishment`), and two runs in total.

### [Motor](http://www.cognitiveatlas.org/task/id/trm_550b53d7dd674/)
`motor` duration: approximately 3 minutes. This task was adapted from (Buckner et al. 2011; Yeo et al. 2011). Participants were presented a visual cue, and were asked to either tap their left or right fingers (event types `left_hand` and `right_hand`, resp.), squeeze their left or right toes (event types `left_foot` and `right_foot`, resp.), or move their tongue to map motor area (event type `tongue`). Each movement lasted 12 seconds, and in total there were 13 blocks, with 2 of `tongue` movements, 4 of hand movements (2 `right_hand` and 2 `left_hand`), and 4 of foot movements (2 `right_foot` and 2 `left_foot`), and three 15 second fixation blocks where participants were instructed not to move anything. There were two runs in total, and 13 blocks per run.

### [Language processing](http://www.cognitiveatlas.org/task/id/trm_550b54a8b30f4/)
`language` duration: approximately 4 minutes. Participants were presented with two types of events. During `story` events, participants listened to an auditory story (5-9 sentences, about 20 seconds), followed by a two-alternative forced-choice question. During `math` events, they listened to a math problem (addition and subtraction only, varies in length), and were instructed to push a button to select the first or the second answer as being correct. The task was adaptive so that for every correct answer the level of difficulty increased. The math task was designed this way to maintain the same level of difficulty between participants. There were 2 runs, each with 4 `story` and 4 `math` blocks, interleaved.

### [Social cognition](http://www.cognitiveatlas.org/task/id/trm_550b557e5f90e/)
`social` duration: approximately 3 minutes. Participants were presented with short video clips (20 seconds) of objects (squares, circles, triangles) that either interacted in some way (event type `mental`), or moved randomly on the screen (event type `random`) ([Castelli et al. 2000](https://doi.org/10.1006/nimg.2000.0612); [Wheatley et al. 2007](https://doi.org/10.1111/j.1467-9280.2007.01923.x)). Following each clip, participants were asked to judge whether the objects had a â€œMental interactionâ€ (an interaction that appeared as if the shapes were taking into account each otherâ€™s feelings and thoughts), whether the were â€œNot Sureâ€, or if there was â€œNo interactionâ€. Button presses were used to record their responses. In each of the two runs, participants viewed 5 `mental` videos and 5 `random` videos, and had 5 fixation blocks of 15 seconds each.

### [Relational processing](http://www.cognitiveatlas.org/task/id/trm_550b5a47aa23e/)
`relational` duration: approximately 3 minutes. Participants were shown 6 different shapes filled with 1 of 6 different textures (Smith et al. 2007). There were two conditions: relations processing (event type `relational`), and control matching condition (event type `control`). In the `relational` events, 2 pairs of objects were presented on the screen, with one pair at the top of the screen, and the other pair at the bottom. Participants were instructed to decide what dimension differed in the top pair (shape or texture), and then decide if the bottom pair differed, or not, on the same dimension (i.e. if the top pair differed in shape, did the bottom pair also differ in shape). Their answers were recorded by one of two button presses: â€œaâ€ differ on same dimension; â€œbâ€ don't differ on same dimension. In the `control` events, participants were shown two objects at the top of the screen, and one object at the bottom of the screen, with a word in the middle of the screen (either â€œshapeâ€ or â€œtextureâ€).They were told to decide whether the bottom object matched either of the top two objects on that dimension (i.e., if the word is â€œshapeâ€, did the bottom object have the same shape as either of the top two objects). Participants responded â€œyesâ€ or â€œnoâ€ using the button box. For the `relational` condition, the stimuli were presented for 3500 ms, with a 500 ms ITI, and there were four trials per block. In the `control`condition, stimuli were presented for 2800 ms, with a 400 ms ITI, and there were 5 trials per block. In total there were two runs, each with three `relational` blocks, three `control` blocks and three 16-second fixation blocks.

### [Emotion processing](http://www.cognitiveatlas.org/task/id/trm_550b5b066d37b/)
`emotion` duration: approximately 2 minutes. Participants were shown triads of faces (event type `face`) or shapes (event type `shape`), and were asked to decide which of the shapes at the bottom of the screen matches the target face/ shape at the top of the screen (adapted from Smith et al. 2007). Faces had either an angry or fearful expression. Faces, and shapes were presented in three blocks of 6 trials (3 `face` and 3 `shape`), with each trial lasting 2 seconds, followed by a 1 second inter-stimulus interval. Each block was preceded by a 3000 ms task cue (â€œshapeâ€ or â€œfaceâ€), so that each block was 21 seconds long, including the cue. In total there were two runs, three `face` blocks and three `shape` blocks, with 8 seconds of fixation at the end of each run.

### [Working memory](http://www.cognitiveatlas.org/task/id/trm_550b50095d4a3/)
`wm` duration: approximately 5 minutes. There were two subtasks: a category specific representation, and a working memory task. Participants were presented with blocks of either places, tools, faces, and body parts. Within each run, all 4 types of stimuli were presented in block, with each block being labelled as a 2-back task (participants needed to indicate if they saw the same image two images back), or a version of a 0-back task (participants were shown a target at the start of the trial and they needed to indicate if the image that they were seeing matched the target). There were thus 8 different event types `<stim>_<back>`, where `<stim>` was one of `place`, `tools`, `face` or `body`, and `<back>` was one of `0back` or `2back`. Each image was presented for 2 seconds, followed by a 500 ms ITI. Stimuli were presented for 2 seconds, followed by a 500 ms inter-task interval. Each of the 2 runs included 8 event types with 10 trials per type, as well as 4 fixations blocks (15 secs).

### [Resting state](http://www.cognitiveatlas.org/task/id/trm_4c8a834779883/)
`restingstate` duration: 15 minutes. In every other session, one resting-state fMRI run was acquired, giving 5 runs per participant. Participants were asked to have their eye open, be looking at fixation cross in the middle of the screen and be instructed to not fall asleep. A total of five resting-state fMRI runs were acquired per subject.
